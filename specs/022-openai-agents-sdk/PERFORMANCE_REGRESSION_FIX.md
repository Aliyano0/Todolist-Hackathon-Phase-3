# Performance Regression Analysis and Fix

**Issue**: Response time increased from 11s to 23-25s after adding priority/category update support

**Date**: 2026-02-13
**Status**: âš ï¸ CRITICAL - Performance regression identified and partially fixed

---

## Performance Timeline

| Change | Response Time | Delta |
|--------|--------------|-------|
| Baseline (021-ai-chatbot) | ~11s | - |
| After optimizations (reduced history 20â†’10) | ~5-7s (expected) | -45% to -55% |
| After priority/category fix | 23-25s | +109% to +127% âš ï¸ |
| After instruction optimization | TBD (testing required) | Expected: -30% to -40% |

---

## Root Cause Analysis

### 1. Verbose Agent Instructions (PRIMARY CAUSE)

**Before Fix**:
- Instructions: ~1,400 tokens
- Included verbose examples with full conversations
- Repeated explanations and guidelines
- Sent with EVERY request to OpenRouter API

**After Fix**:
- Instructions: ~550 tokens (60% reduction)
- Concise examples with arrow notation
- Streamlined guidelines
- Same functionality, less verbosity

**Impact**: ~850 tokens saved per request
**Expected Improvement**: 30-40% faster processing

### 2. Expanded Tool Schema

**Change**: Added 2 optional parameters to update_task tool
- `priority: Optional[str]` with validation
- `category: Optional[str]`

**Impact**:
- Tool schema sent to LLM increased by ~100 tokens
- LLM must process more complex tool definitions
- Minimal impact (~5% slower)

### 3. Development Mode Server

**Current**: Server running with `--reload` flag
- File watching overhead
- Auto-reload on code changes
- Not optimized for performance

**Impact**: 10-15% slower than production mode

---

## Optimizations Applied

### âœ… Completed

1. **Reduced Conversation History** (20 â†’ 10 messages)
   - File: `chat_service.py`, `chat.py`
   - Impact: 50% less history tokens

2. **Optimized Database Query** (2 queries â†’ 1 JOIN)
   - File: `chat_service.py`
   - Impact: Faster database operations

3. **Singleton AgentService** (per-request â†’ module-level)
   - File: `chat.py`
   - Impact: No initialization overhead

4. **Streamlined Agent Instructions** (1,400 â†’ 550 tokens)
   - File: `agent_service.py`
   - Impact: 60% reduction in instruction tokens

### â³ Pending Testing

Need to restart server and measure actual performance improvement.

---

## Additional Optimization Opportunities

### High Priority (Immediate)

1. **Run Server in Production Mode**
   ```bash
   # Instead of: uv run uvicorn main:app --reload
   # Use: uv run uvicorn main:app --workers 1
   ```
   Expected: 10-15% improvement

2. **Further Reduce Conversation History** (10 â†’ 5 messages)
   - Most conversations don't need 10 messages of context
   - 5 messages is sufficient for task management
   Expected: 20-30% improvement

3. **Cache Agent Instance** (already done, verify it's working)
   - Ensure agent is truly initialized once at module load
   - No re-initialization on each request

### Medium Priority (If Still Slow)

4. **Simplify Tool Descriptions**
   - Current tool docstrings are verbose
   - LLM processes these on every request
   - Reduce to essential information only

5. **Remove Unused Tool Parameters**
   - Review if all parameters are necessary
   - Consider removing rarely-used optional parameters

6. **Implement Response Streaming**
   - Stream agent responses as they're generated
   - Improves perceived performance
   - Requires frontend changes

### Low Priority (Future)

7. **Model Selection**
   - Use faster model for simple queries
   - Reserve gpt-4o-mini for complex operations

8. **Conversation Summarization**
   - Summarize old messages instead of sending full text
   - Reduces token count while preserving context

9. **Redis Caching**
   - Cache conversation history for active users
   - Reduce database queries

---

## Recommended Action Plan

### Step 1: Restart Server (IMMEDIATE)
```bash
cd backend
# Stop current server (Ctrl+C)
# Start without --reload flag
uv run uvicorn main:app --host 0.0.0.0 --port 8000
```

### Step 2: Test Performance
Send a test message and measure response time:
```bash
time curl -X POST "http://localhost:8000/api/{user_id}/chat" \
  -H "Authorization: Bearer {token}" \
  -H "Content-Type: application/json" \
  -d '{"message": "change task 1 priority to high", "conversation_id": "{conv_id}"}'
```

### Step 3: If Still Slow (>10s), Apply Additional Optimizations

**Option A: Reduce history to 5 messages**
```python
# In chat.py and chat_service.py
limit=5  # Change from 10 to 5
```

**Option B: Simplify tool descriptions**
```python
# In agent_service.py, make docstrings more concise
"""Update task fields."""  # Instead of verbose descriptions
```

### Step 4: Monitor and Iterate

Track these metrics:
- Response time (p50, p95, p99)
- Token usage (input tokens per request)
- Database query time
- OpenRouter API latency

---

## Expected Performance After All Fixes

| Scenario | Current | Target | Improvement |
|----------|---------|--------|-------------|
| Simple query (no history) | 23-25s | <5s | 80% faster |
| Medium query (5 messages) | 23-25s | <7s | 70% faster |
| Complex query (10 messages) | 23-25s | <10s | 60% faster |

---

## Performance Debugging Commands

### Check Server Mode
```bash
ps aux | grep uvicorn
# Look for --reload flag (bad for performance)
```

### Monitor Response Times
```bash
# Watch logs for timing information
tail -f logs/app.log | grep "ðŸ”„\|âœ…"
```

### Measure Token Usage
```bash
# Check OpenRouter dashboard for token counts
# Compare before/after optimization
```

### Profile Database Queries
```python
# Add timing logs in chat_service.py
import time
start = time.time()
# ... query ...
logger.info(f"Query took {time.time() - start:.2f}s")
```

---

## Rollback Plan

If performance doesn't improve, revert changes:

```bash
# Revert agent instructions
git checkout HEAD~1 backend/core/services/agent_service.py

# Revert update_task changes
git checkout HEAD~2 backend/mcp_server/tools/update_task.py
git checkout HEAD~2 backend/core/services/agent_service.py
```

---

## Success Criteria

- [ ] Response time <10s for queries with history
- [ ] Response time <5s for simple queries
- [ ] Token usage reduced by 50% from current
- [ ] Priority/category updates still work correctly
- [ ] No functionality regressions

---

## Next Steps

1. **IMMEDIATE**: Restart server without --reload flag
2. **TEST**: Measure response time with optimized instructions
3. **IF NEEDED**: Apply additional optimizations (reduce history to 5)
4. **MONITOR**: Track performance metrics over time
5. **DOCUMENT**: Update performance benchmarks

---

**Status**: Instruction optimization applied, awaiting server restart and testing
**Priority**: CRITICAL - Performance regression must be resolved
**Owner**: Backend team
**ETA**: Should be resolved within 1 hour of applying fixes
